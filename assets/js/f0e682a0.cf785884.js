"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[881],{3897:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"What is BigHummingbird","href":"/documentation/docs/intro","docId":"intro","unlisted":false},{"type":"category","label":"Quick Start","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Evaluating your model","href":"/documentation/docs/quick_start/create_a_random_judge","docId":"quick_start/create_a_random_judge","unlisted":false},{"type":"link","label":"LLM as a judge","href":"/documentation/docs/quick_start/llm_as_a_judge","docId":"quick_start/llm_as_a_judge","unlisted":false}],"href":"/documentation/docs/category/quick-start"},{"type":"category","label":"Concepts","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Run","href":"/documentation/docs/concepts/run","docId":"concepts/run","unlisted":false},{"type":"link","label":"Model","href":"/documentation/docs/concepts/model","docId":"concepts/model","unlisted":false},{"type":"link","label":"Dataset","href":"/documentation/docs/concepts/dataset","docId":"concepts/dataset","unlisted":false},{"type":"link","label":"Evaluations","href":"/documentation/docs/concepts/evaluations","docId":"concepts/evaluations","unlisted":false},{"type":"link","label":"Judge","href":"/documentation/docs/concepts/judge","docId":"concepts/judge","unlisted":false}],"href":"/documentation/docs/category/concepts"}]},"docs":{"concepts/dataset":{"id":"concepts/dataset","title":"Dataset","description":"A \\"dataset\\" is a collection of input data points, arranged in an array, used to evaluate and test the performance and accuracy of a model. Think of it as a set of different test scenarios to see how well your model performs with various inputs.","sidebar":"tutorialSidebar"},"concepts/evaluations":{"id":"concepts/evaluations","title":"Evaluations","description":"An \\"evaluation\\" is similar to a run and is created when the @assess decorator is called.","sidebar":"tutorialSidebar"},"concepts/judge":{"id":"concepts/judge","title":"Judge","description":"A \\"judge\\" consists of a scoring function that assigns a numeric score to the outputs.","sidebar":"tutorialSidebar"},"concepts/model":{"id":"concepts/model","title":"Model","description":"A \\"model\\" is a component that keeps tabs on the input and output types along with the code it runs.","sidebar":"tutorialSidebar"},"concepts/run":{"id":"concepts/run","title":"Run","description":"A \\"run\\" is generated when the @assess decorator is called.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"What is BigHummingbird","description":"Hummingbird is a simple toolbox to help speed up your LLM projects during experimental phase so you can focus on the tweaking and not the tracking and get your models to production faster.","sidebar":"tutorialSidebar"},"quick_start/create_a_random_judge":{"id":"quick_start/create_a_random_judge","title":"Evaluating your model","description":"Imagine that you\'re creating a model that crafts responses for customer support and you want to know how your model performs against a fixed set of data points.","sidebar":"tutorialSidebar"},"quick_start/llm_as_a_judge":{"id":"quick_start/llm_as_a_judge","title":"LLM as a judge","description":"","sidebar":"tutorialSidebar"}}}}')}}]);