"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[732],{8444:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>r,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var t=n(4848),s=n(8453);const i={sidebar_position:6},o="Evaluations",l={id:"concepts/evaluations",title:"Evaluations",description:"Evaluations play a critical role in assessing the performance of a model against a predefined dataset. By systematically measuring a model's accuracy, we can gain valuable insights into its strengths and areas for improvement. In this section, we will guide you through the process of creating and understanding evaluations in our system.",source:"@site/docs/concepts/evaluations.md",sourceDirName:"concepts",slug:"/concepts/evaluations",permalink:"/llm/docs/concepts/evaluations",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Judge",permalink:"/llm/docs/concepts/judge"},next:{title:"Further Topics",permalink:"/llm/docs/category/further-topics"}},r={},c=[{value:"Creating an Evaluation",id:"creating-an-evaluation",level:2},{value:"Example Usage",id:"example-usage",level:3},{value:"Understanding Evaluations",id:"understanding-evaluations",level:2},{value:"Components of an Evaluation",id:"components-of-an-evaluation",level:3},{value:"Viewing Evaluation Results",id:"viewing-evaluation-results",level:3}];function d(e){const a={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.h1,{id:"evaluations",children:"Evaluations"}),"\n",(0,t.jsx)(a.p,{children:"Evaluations play a critical role in assessing the performance of a model against a predefined dataset. By systematically measuring a model's accuracy, we can gain valuable insights into its strengths and areas for improvement. In this section, we will guide you through the process of creating and understanding evaluations in our system."}),"\n",(0,t.jsx)(a.h2,{id:"creating-an-evaluation",children:"Creating an Evaluation"}),"\n",(0,t.jsxs)(a.p,{children:["Evaluations are created using the ",(0,t.jsx)(a.code,{children:"@bhb.assess"})," decorator. This decorator requires 2 parameters."]}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.code,{children:"judge_tag"}),": the tag identifying the judge's scoring function"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.code,{children:"dataset_tag"}),": the tag identifying the dataset to be used for the evaluation"]}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,t.jsxs)(a.p,{children:["To create an evaluation, simply call the ",(0,t.jsx)(a.code,{children:"@bhb.assess"})," decorator with the appropriate tags:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"@bhb.assess(judge_tag='accuracy_scoring_judge:v1', dataset_tag='validation_set:v2')\ndef model():\n    # Your model implementation here\n    pass\nmodel()\n"})}),"\n",(0,t.jsxs)(a.p,{children:["In this example, the evaluation uses the ",(0,t.jsx)(a.code,{children:"accuracy_scoring_judge:v1"})," as the judge and the ",(0,t.jsx)(a.code,{children:"validation_set"})," as the dataset."]}),"\n",(0,t.jsx)(a.admonition,{title:"Evaluation",type:"tip",children:(0,t.jsx)(a.p,{children:"Evaluation are only calculated and created when you make a call to a model."})}),"\n",(0,t.jsx)(a.h2,{id:"understanding-evaluations",children:"Understanding Evaluations"}),"\n",(0,t.jsxs)(a.p,{children:["An evaluation measures how well a model performs against a predefined dataset by using a ",(0,t.jsx)(a.a,{href:"/llm/docs/concepts/judge",children:"judge component"}),". This process involves comparing the model's predictions to the actual values in the dataset and calculating a performance score."]}),"\n",(0,t.jsx)(a.h3,{id:"components-of-an-evaluation",children:"Components of an Evaluation"}),"\n",(0,t.jsxs)(a.ol,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsxs)(a.strong,{children:["Judge component (",(0,t.jsx)(a.code,{children:"judge_tag"}),")"]}),": this function defines the criteria for measuring the model's performance. You can also optionally include a passing function. Common scoring functions include accuracy, precision, recall, and F1-score."]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsxs)(a.strong,{children:["Dataset(",(0,t.jsx)(a.code,{children:"dataset_tag"}),")"]}),": the ",(0,t.jsx)(a.a,{href:"/llm/docs/concepts/dataset",children:"dataset"})," is a collection of data points used to evaluate the model. It typically includes input features and corresponding true labels. The ",(0,t.jsx)(a.code,{children:"dataset_tag"})," parameter identifies which dataset to use for the evaluation."]}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"viewing-evaluation-results",children:"Viewing Evaluation Results"}),"\n",(0,t.jsxs)(a.p,{children:["To view the results fo an evaluation, make a call to your model with the decorator and BigHummingbird will generate a detail link to the evaluation and take you to the dashboard.\n",(0,t.jsx)(a.img,{alt:"evaluation_detail",src:n(8341).A+"",width:"926",height:"763"})]})]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8341:(e,a,n)=>{n.d(a,{A:()=>t});const t=n.p+"assets/images/evaluation_detail-fa30b063532ce7b7a783b1fe4f315b0f.png"},8453:(e,a,n)=>{n.d(a,{R:()=>o,x:()=>l});var t=n(6540);const s={},i=t.createContext(s);function o(e){const a=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(i.Provider,{value:a},e.children)}}}]);