"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[131],{7978:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>l,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var a=n(4848),s=n(8453);const i={sidebar_position:2},o="Evaluating your model",r={id:"quick_start/evaluating_model",title:"Evaluating your model",description:"Imagine that you're creating a model that crafts responses for customer support and you want to know how your model performs against a fixed set of data points.",source:"@site/docs/quick_start/evaluating_model.md",sourceDirName:"quick_start",slug:"/quick_start/evaluating_model",permalink:"/documentation/docs/quick_start/evaluating_model",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/quick_start/evaluating_model.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Quick Start",permalink:"/documentation/docs/category/quick-start"},next:{title:"LLM as a judge",permalink:"/documentation/docs/quick_start/llm_as_a_judge"}},d={},c=[{value:"Define your own dataset",id:"define-your-own-dataset",level:2},{value:"Initialize your project",id:"initialize-your-project",level:3},{value:"Create a static model",id:"create-a-static-model",level:3},{value:"Create your dataset",id:"create-your-dataset",level:3},{value:"Upload your dataset",id:"upload-your-dataset",level:3},{value:"View your dataset on the dashboard.",id:"view-your-dataset-on-the-dashboard",level:3},{value:"How the dataset is versioned?",id:"how-the-dataset-is-versioned",level:3},{value:"Create your own judge",id:"create-your-own-judge",level:2},{value:"Define your scoring function",id:"define-your-scoring-function",level:3},{value:"Define your passing criteria (optional)",id:"define-your-passing-criteria-optional",level:3},{value:"Put scoring function and passing criteria together",id:"put-scoring-function-and-passing-criteria-together",level:3},{value:"Perfect!",id:"perfect",level:2},{value:"Putting everything together",id:"putting-everything-together",level:2},{value:"Next steps",id:"next-steps",level:2}];function u(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"evaluating-your-model",children:"Evaluating your model"}),"\n",(0,a.jsx)(t.p,{children:"Imagine that you're creating a model that crafts responses for customer support and you want to know how your model performs against a fixed set of data points."}),"\n",(0,a.jsx)(t.h2,{id:"define-your-own-dataset",children:"Define your own dataset"}),"\n",(0,a.jsx)(t.h3,{id:"initialize-your-project",children:"Initialize your project"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Install bighummingbird python package"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"pip install bighummingbird\n"})}),"\n",(0,a.jsxs)(t.p,{children:["In your python code, create an instance of ",(0,a.jsx)(t.code,{children:"Bighummingbird"}),"."]}),"\n",(0,a.jsx)(t.admonition,{title:"API Key",type:"tip",children:(0,a.jsxs)(t.p,{children:["You can get your API_KEY at ",(0,a.jsx)(t.a,{href:"http://www.bighummingbird.com",children:"www.bighummingbird.com"})," Workspace > Settings > Create New API Key"]})}),"\n",(0,a.jsx)(t.admonition,{title:"Don't want to expose your API_KEYs?",type:"tip",children:(0,a.jsxs)(t.p,{children:["You can store your API_KEYs in your local ",(0,a.jsx)(t.code,{children:".env"})," file for security. See securely store your API_KEYs for more information."]})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from bighummingbird import BigHummingbird\nbhb = BigHummingbird("Customer support project", API_KEY)\n'})}),"\n",(0,a.jsx)(t.h3,{id:"create-a-static-model",children:"Create a static model"}),"\n",(0,a.jsx)(t.p,{children:"To begin with, we'll create a static model that returns the same response for any input. Let's say the model just returns \"What do you mean?\", which is not a very good response! Let's see how we can evaluate the performance of this model."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'def sample_model(question):\n    # Feel free to swap this out to use your custom LLM model.\n    return {\n        "question": question,\n        "answer:: "What do you mean?",\n    }\n'})}),"\n",(0,a.jsx)(t.h3,{id:"create-your-dataset",children:"Create your dataset"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"Dataset"})," is a collection of only input data points, arranged in an array."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from bighummingbird.dataset import Dataset\ncustomer_questions = [\n    'I recently bought a blender from your store, but it stopped working within a week.',\n    'I\\'m having trouble installing the software I purchased. Can you help?',\n    'My order was supposed to arrive last week but I haven\\'t received it yet. What\\'s happening?',\n    'I\\'m locked out of my account and can\\'t reset my password. What should I do?',\n    'The air purifier I received last week is making a strange noise when it\\'s on the highest setting. Should I be concerned or is this normal?'\n]\n\ndataset = Dataset(\"sample-test-dataset\", data=customer_questions, description=\"Customer questions\")\ndataset_tag = bhb.upload_dataset(dataset)\n"})}),"\n",(0,a.jsx)(t.h3,{id:"upload-your-dataset",children:"Upload your dataset"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"dataset_tag = bhb.upload_dataset(dataset)\n"})}),"\n",(0,a.jsx)(t.h3,{id:"view-your-dataset-on-the-dashboard",children:"View your dataset on the dashboard."}),"\n",(0,a.jsxs)(t.p,{children:["Variable ",(0,a.jsx)(t.code,{children:"dataset_tag"})," will be how you reference this particular dataset. In this example, it is ",(0,a.jsx)(t.code,{children:"sample-test-dataset:v1"})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.img,{alt:"dataset_image",src:n(9930).A+"",width:"1698",height:"460"}),"\n",(0,a.jsx)(t.img,{alt:"test_dataset_detail_modal_image",src:n(4710).A+"",width:"1088",height:"502"})]}),"\n",(0,a.jsx)(t.h3,{id:"how-the-dataset-is-versioned",children:"How the dataset is versioned?"}),"\n",(0,a.jsxs)(t.p,{children:["Any changes to the inputs, whether the value or the structure will automatically trigger a version update. Check ",(0,a.jsx)(t.a,{href:"/documentation/docs/concepts/dataset",children:"Dataset"})," for more information."]}),"\n",(0,a.jsx)(t.h2,{id:"create-your-own-judge",children:"Create your own judge"}),"\n",(0,a.jsxs)(t.p,{children:["A ",(0,a.jsx)(t.code,{children:"Judge"})," consists of a scoring function that assigns a numeric score to the outputs."]}),"\n",(0,a.jsx)(t.h3,{id:"define-your-scoring-function",children:"Define your scoring function"}),"\n",(0,a.jsxs)(t.p,{children:["A ",(0,a.jsx)(t.code,{children:"scoring_function"})," is a function that takes your model outputs, and give them a numeric score."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"def scoring_rubric(output):\n    import random\n    return random.randint(1, 10)\n"})}),"\n",(0,a.jsx)(t.admonition,{title:"Tip",type:"tip",children:(0,a.jsxs)(t.p,{children:["Notice how ",(0,a.jsx)(t.code,{children:"import random"})," is added within a function. This is important because we track the entire function definition and run it against the model later on. If you have any dependencies, you must import it within the function."]})}),"\n",(0,a.jsxs)(t.p,{children:["In this example, the scoring rubric simply gives out a score between 1 to 10. Not very helpful in evaluating our model, but this fine for our demo purpose. To see how we could use LLM-as-a-judge to evaluate our model, see ",(0,a.jsx)(t.a,{href:"/documentation/docs/quick_start/llm_as_a_judge",children:"LLM-as-a-judge with OpenAI"})]}),"\n",(0,a.jsx)(t.h3,{id:"define-your-passing-criteria-optional",children:"Define your passing criteria (optional)"}),"\n",(0,a.jsxs)(t.p,{children:["Taken the score from ",(0,a.jsx)(t.code,{children:"scoring_rubric"}),", you can define a passing or failing threshold so that you can visually see your model's performance on the dashboard."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"def passing_criteria(score):\n    return score > 5\n"})}),"\n",(0,a.jsxs)(t.p,{children:["You can also define ",(0,a.jsx)(t.code,{children:"passing_criteria"})," as a range like. Just make sure that the output is a ",(0,a.jsx)(t.code,{children:"bool"})," type."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"def passing_criteria(score):\n    return 5 < score and score < 9\n"})}),"\n",(0,a.jsx)(t.h3,{id:"put-scoring-function-and-passing-criteria-together",children:"Put scoring function and passing criteria together"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from bighummingbird.judge import Judge\njudge = Judge(\n    "random-judge",\n    "This judge will return a random score between 1 to 10",\n    scoring_rubric,\n    passing_criteria,\n)\n'})}),"\n",(0,a.jsx)(t.p,{children:"and add upload the judge to bighummingbird"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"judge_tag = bhb.add_judge(judge)\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"random_judge_detail_model",src:n(1454).A+"",width:"1091",height:"379"})}),"\n",(0,a.jsx)(t.h2,{id:"perfect",children:"Perfect!"}),"\n",(0,a.jsx)(t.p,{children:"Now we can run our model and view the evaluation score on bighummingbird dashboard!"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="sample_model.py"',children:'@bhb.assess("random-judge:v1", "sample-test-dataset:v1")\ndef sample_model(question):\n    return {\n        "question": question,\n        "answer": "What do you mean?"\n    }\n    \nsample_model("I recently bought a blender from your store, but it stopped working within a week.")\n'})}),"\n",(0,a.jsx)(t.p,{children:"Run the modal"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"python sample_model.py\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"\u2714 Project set to: Customer support project\n\n=== Start of Evaluation ===\n\n\u2716 sample-test-dataset[0] score: 4\n\u2716 sample-test-dataset[1] score: 3\n\u2714 sample-test-dataset[2] score: 7\n\u2716 sample-test-dataset[3] score: 5\n\u2714 sample-test-dataset[4] score: 8\n\n\u2716 3 out of 5 failed.\n\n=== End of Evaluation ===\n\n\u2714 Model sample_model:v1 uploaded.\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"evaluation_detail_modal",src:n(5916).A+"",width:"1085",height:"659"})}),"\n",(0,a.jsx)(t.admonition,{title:"Remove assessment for production",type:"tip",children:(0,a.jsx)(t.p,{children:"Remember to remove the assessment decorator for production. Because bighummingbird will run the model against each data on your dataset, it might cause un-necessary computation and slow things down in production."})}),"\n",(0,a.jsx)(t.h2,{id:"putting-everything-together",children:"Putting everything together"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from bighummingbird import BigHummingbird\nfrom bighummingbird.judge import Judge\nfrom bighummingbird.dataset import Dataset\n\nbhb = BigHummingbird("Customer support project", API_KEY)\n\ncustomer_questions = [\n    \'I recently bought a blender from your store, but it stopped working within a week.\',\n    \'I\\\'m having trouble installing the software I purchased. Can you help?\',\n    \'My order was supposed to arrive last week but I haven\\\'t received it yet. What\\\'s happening?\',\n    \'I\\\'m locked out of my account and can\\\'t reset my password. What should I do?\',\n    \'The air purifier I received last week is making a strange noise when it\\\'s on the highest setting. Should I be concerned or is this normal?\'\n]\n\ndataset = Dataset("sample-test-dataset", data=customer_questions, description="Customer questions")\ndataset_tag = bhb.upload_dataset(dataset)\n\ndef scoring_rubric(outputs):\n    import random\n    return random.randint(1, 10)\n\ndef passing_criteria(score):\n    return score > 5\n\njudge = Judge(\n    "random-judge",\n    "This judge will return a random score between 1 to 10",\n    scoring_rubric,\n    passing_criteria,\n)\njudge_tag = bhb.add_judge(judge)\n\n@bhb.assess(judge_tag, dataset_tag)\ndef sample_model(question):\n    return {\n        "question": question,\n        "answer": "What do you mean?"\n    }\nsample_model("This is a test question")\n'})}),"\n",(0,a.jsx)(t.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["Learn how to add ",(0,a.jsx)(t.a,{href:"/documentation/docs/quick_start/llm_as_a_judge",children:"LLM-as-a-judge with OpenAI"})]}),"\n"]})]})}function l(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},9930:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/dataset_table-b23b416751056263b0939aea66ef7188.png"},5916:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/evaluation_sample_detail_modal-1295aff30c44ff5079238f07d6e6276b.png"},1454:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/random_judge_detail_modal-2b0e6982142dceafcb7564600e7132df.png"},4710:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/test_dataset_detail_modal-e048ad93d417d0d5675f296a673dcaab.png"},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>r});var a=n(6540);const s={},i=a.createContext(s);function o(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);