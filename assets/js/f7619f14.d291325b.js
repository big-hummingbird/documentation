"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[691],{7290:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>l,frontMatter:()=>o,metadata:()=>s,toc:()=>u});var i=t(4848),r=t(8453);const o={sidebar_position:2},a="Run",s={id:"concepts/run",title:"Run",description:"A model run includes tracking the inputs, outputs, and latency of executing a trained machine learning model with specific data to generate predictions.",source:"@site/docs/concepts/run.md",sourceDirName:"concepts",slug:"/concepts/run",permalink:"/documentation/docs/concepts/run",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/concepts/run.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Concepts",permalink:"/documentation/docs/category/concepts"},next:{title:"Model",permalink:"/documentation/docs/concepts/model"}},d={},u=[{value:"Why trace model run inputs and outputs?",id:"why-trace-model-run-inputs-and-outputs",level:2},{value:"Tracing a model run",id:"tracing-a-model-run",level:2},{value:"Putting it all together",id:"putting-it-all-together",level:3},{value:"Evaluate model run performance",id:"evaluate-model-run-performance",level:2}];function c(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"run",children:"Run"}),"\n",(0,i.jsx)(n.p,{children:"A model run includes tracking the inputs, outputs, and latency of executing a trained machine learning model with specific data to generate predictions."}),"\n",(0,i.jsx)(n.h2,{id:"why-trace-model-run-inputs-and-outputs",children:"Why trace model run inputs and outputs?"}),"\n",(0,i.jsx)(n.p,{children:"Tracing every run input and output is crucial throughout the model lifecycle, from experimentation to production, for several reasons:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Debugging and Error Analysis"}),": Identifies where a model fails or produces unexpected results."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance monitoring"}),": Detects performance degradation, anomalies, or drift by analyzing real-time input and output data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optimizing Model Performance"}),": Identifies patterns and trends for further optimizations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Handling Non-Deterministic Models"}),": Understands and quantifies variability in models that produce different outputs for the same input."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"tracing-a-model-run",children:"Tracing a model run"}),"\n",(0,i.jsx)(n.p,{children:"BigHummingbird makes it simple to trace a model run input and output values."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="model.py"',children:'from bighummingbird import BigHummingbird\nbhb = BigHummingbird("Concept Project", API_KEY)\n\n# Add the trace decorator\n@bhb.trace\ndef model(input_a, input_b):\n  # Your model here\n  # GPT-4 text generation\n  # summarization\n  # translation\n  # sentiment analysis\n  return input_a + input_b\n\nmodel(1, 2)\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python model.py\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"View runs and models on a dashboard"}),(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.img,{alt:"run_table",src:t(6865).A+"",width:"1699",height:"462"}),"\n",(0,i.jsx)(n.img,{alt:"model_detail_v1",src:t(2372).A+"",width:"1088",height:"372"})]}),"\n",(0,i.jsxs)(n.p,{children:["This will automatically track your model function signature, outputs, and the model definition. Any changes to these attributes will automatically trigger BigHummingbird to increment your model version. Read ",(0,i.jsx)(n.a,{href:"/documentation/docs/concepts/model",children:"Model"})," for more information"]}),"\n",(0,i.jsx)(n.h3,{id:"putting-it-all-together",children:"Putting it all together"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="model.py"',children:'from bighummingbird import BigHummingbird\nfrom bighummingbird.judge import Judge\n\nbhb = BigHummingbird("Quick Start", API_KEY)\n\ndef scoring_rubric(outputs):\n    # This is important. All necessary imports must be done here.\n    import random\n    return random.randint(1, 10)\n\ndef passing_criteria(score):\n    return score > 5\n\njudge = Judge(\n    "random-judge",\n    "This judge will return a random score between 1 to 10",\n    scoring_rubric,\n    passing_criteria,\n)\njudge_tag = bhb.add_judge(judge)\n\n@bhb.assess(judge_tag)\ndef model(input_a, input_b):\n  # Your model here\n  # GPT-4 text generation\n  # summarization\n  # translation\n  # sentiment analysis\n  return input_a + input_b\n'})}),"\n",(0,i.jsx)(n.h2,{id:"evaluate-model-run-performance",children:"Evaluate model run performance"})]})}function l(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},2372:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/model_detail_v1-0311113f12a77b96250af893e2c896f3.png"},6865:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/run_table-3cf816043c4c1a4b90d71e3a50f89464.png"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var i=t(6540);const r={},o=i.createContext(r);function a(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);